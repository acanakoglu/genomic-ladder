{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#This is the modified version of the ladder network code from https://github.com/rinuboney/ladder\n",
    "#Certain modfications are made to use & experiment with gene expression data\n",
    "#\n",
    "# import pickle\n",
    "import numpy as np\n",
    "import numpy \n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "#import input_data\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "join = lambda l, u: tf.concat([l, u], 0)\n",
    "labeled = lambda x: tf.slice(x, [0, 0], [batch_size, -1]) if x is not None else x\n",
    "unlabeled = lambda x: tf.slice(x, [batch_size, 0], [-1, -1]) if x is not None else x\n",
    "split_lu = lambda x: (labeled(x), unlabeled(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brca\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'out/brca.tsv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cancer_type = 'aggregates/KIDNEY'\n",
    "import sys\n",
    "cancer_type = sys.argv[1]\n",
    "if cancer_type.startswith('-'):\n",
    "    cancer_type = 'BRCA'\n",
    "cancer_type_file = cancer_type.replace(\"/\",\"_\").lower()\n",
    "print(cancer_type_file)\n",
    "file = 'out/' + cancer_type_file +  \".tsv\"\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#class definitions\n",
    "\n",
    "class DataSet(object):\n",
    "\n",
    "  def __init__(self, dataset, labels):\n",
    "    \n",
    "    self._dataset = dataset\n",
    "    self._labels = labels\n",
    "    self._epochs_completed = 0\n",
    "    self._index_in_epoch = 0\n",
    "    self._num_examples = dataset.shape[0]\n",
    "\n",
    "  @property\n",
    "  def dataset(self):\n",
    "    return self._dataset\n",
    "\n",
    "  @property\n",
    "  def labels(self):\n",
    "    return self._labels\n",
    "\n",
    "  @property\n",
    "  def num_examples(self):\n",
    "    return self._num_examples\n",
    "\n",
    "  @property\n",
    "  def epochs_completed(self):\n",
    "    return self._epochs_completed\n",
    "\n",
    "  def next_batch(self, batch_size):\n",
    "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "    start = self._index_in_epoch\n",
    "#     print(start)\n",
    "    end = start + batch_size\n",
    "    \n",
    "    result_data = self._dataset[start:end]\n",
    "    result_label = self._labels[start:end]\n",
    "    \n",
    "    while len(result_data) < batch_size:\n",
    "        # Finished epoch\n",
    "        self._epochs_completed += 1\n",
    "        # Shuffle the data\n",
    "        perm = np.arange(self._num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        self._dataset = self._dataset[perm]\n",
    "        self._labels = self._labels[perm]\n",
    "        # Start next epoch\n",
    "        start = 0\n",
    "        end = batch_size - len(result_data)\n",
    "        result_data = np.append(result_data,self._dataset[start:end], axis=0)\n",
    "        result_label = np.append(result_label,self._labels[start:end], axis=0)\n",
    "    self._index_in_epoch = end\n",
    "#     print(start, end)\n",
    "    return result_data ,result_label\n",
    "\n",
    "class SemiDataSet(object):\n",
    "    def __init__(self, dataset, labels, n_labeled):\n",
    "        \n",
    "        self.n_labeled = n_labeled\n",
    "\n",
    "        # Unlabled DataSet\n",
    "        self.unlabeled_ds = DataSet(dataset, labels)\n",
    "\n",
    "        # Labeled DataSet\n",
    "        self.num_examples = self.unlabeled_ds.num_examples\n",
    "        indices = np.arange(self.num_examples)\n",
    "        shuffled_indices = np.random.permutation(indices)\n",
    "        dataset = dataset[shuffled_indices]\n",
    "        labels = labels[shuffled_indices]\n",
    "#         print('labels',labels)\n",
    "        \n",
    "        y = np.array([np.arange(2)[l==1][0] for l in labels])\n",
    "#         print('y',y)\n",
    "#         global test\n",
    "#         test=labels\n",
    "\n",
    "        \n",
    "#         idx = indices[y==0][:5]\n",
    "#         print('idx',idx)\n",
    "\n",
    "\n",
    "        n_classes = y.max() + 1\n",
    "#         print('n_classes',n_classes)\n",
    "        n_from_each_class = n_labeled // n_classes\n",
    "        i_labeled = []\n",
    "        for c in range(n_classes):\n",
    "            i = indices[y==c][:n_from_each_class]\n",
    "            i_labeled += list(i)\n",
    "        l_dataset = dataset[i_labeled]\n",
    "        l_labels = labels[i_labeled]\n",
    "        self.labeled_ds = DataSet(l_dataset, l_labels)\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        #print (\"batch size semi\", batch_size)\n",
    "        unlabeled_dataset, _ = self.unlabeled_ds.next_batch(batch_size)\n",
    "     \n",
    "        if batch_size > self.n_labeled:\n",
    "            labeled_dataset, labels = self.labeled_ds.next_batch(self.n_labeled)\n",
    "        else:\n",
    "            labeled_dataset, labels = self.labeled_ds.next_batch(batch_size)\n",
    "            #print (labeled_dataset.shape)\n",
    "        #print (\"labels shape aasd\", labels.shape)\n",
    "        #print (labels)\n",
    "        dataset = np.vstack([labeled_dataset, unlabeled_dataset])\n",
    "        return dataset, labels\n",
    "\n",
    "    \n",
    "class SemiDataSet2(object):\n",
    "    def __init__(self, dataset, labels, unlabelled_dataset=None):\n",
    "        if unlabelled_dataset is None:\n",
    "            unlabelled_dataset = dataset[0:0]\n",
    "\n",
    "        assert dataset.shape[1] == unlabelled_dataset.shape[1]\n",
    "        assert dataset.shape[0] == labels.shape[0]\n",
    "\n",
    "        # Unlabled DataSet with dummy labels\n",
    "        unlab_ds = np.vstack([unlabelled_dataset, dataset])\n",
    "        self.unlabeled_ds = DataSet(unlab_ds, np.arange(len(unlab_ds)))\n",
    "\n",
    "        # Labeled DataSet\n",
    "        self.num_examples = dataset.shape[0]\n",
    "        self.labeled_ds = DataSet(dataset, labels)\n",
    "                                    \n",
    "    def next_batch(self, batch_size):\n",
    "        unlabeled_dataset, _ = self.unlabeled_ds.next_batch(batch_size)\n",
    "        labeled_dataset, labels = self.labeled_ds.next_batch(batch_size)\n",
    "        dataset = np.vstack([labeled_dataset, unlabeled_dataset])\n",
    "        return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#one-hot label\n",
    "def dense_to_one_hot(labels_dense, num_classes=2):\n",
    "\n",
    "  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "  num_labels = labels_dense.shape[0]\n",
    "#   print(num_labels)\n",
    "  index_offset = np.arange(num_labels) * num_classes\n",
    "  labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "  return labels_one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#fix labels  1 for tumoral, 0 for healthy\n",
    "def fix_label(labels):\n",
    "    labels= [1 if x==1 else 0 for x in labels]\n",
    "    \n",
    "    return np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = '../nanni_data_tcga_cibb/'+cancer_type+'/'\n",
    "# wd = '/home/nanni/Data/TCGA/CIBB/aggregates/LUNG/'\n",
    "# wd = '/home/nanni/Data/TCGA/CIBB/aggregates/KIDNEY/'\n",
    "\n",
    "\n",
    "X_file = wd + '/X.npy'\n",
    "y_file = wd + '/y.npy'\n",
    "X = np.load(X_file)\n",
    "y = np.load(y_file)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "# X_all_file = '/home/canakoglu/projects/genomic-ladder/X_all.npy'\n",
    "# # X_all_file = 'X_all.npy'\n",
    "# X_all = np.load(X_all_file)\n",
    "# print(X_all.shape)\n",
    "\n",
    "# X_all = X_all[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "print (\"===  Loading Data ===\")\n",
    "\n",
    "\n",
    "class DataSets(object):\n",
    "    pass\n",
    "data_sets = DataSets()\n",
    "\n",
    "\n",
    "X = np.load(X_file)\n",
    "ylabels = np.load(y_file)\n",
    "print(X.shape)\n",
    "print(ylabels.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO remove all zeros\n",
    "# scaler = MinMaxScaler()\n",
    "# Xnew = scaler.fit_transform(X.T).T\n",
    "\n",
    "# Xnew = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Xnew = scaler.fit_transform(X)\n",
    "\n",
    "# Xnew = (X/X.max(axis=1))\n",
    "\n",
    "\n",
    "\n",
    "Xnew = X\n",
    "\n",
    "\n",
    "# Xnew = X.T[X.sum(axis =0) != 0].T\n",
    "\n",
    "print('# of 1s', sum(ylabels))\n",
    "\n",
    "ynew = dense_to_one_hot(fix_label(ylabels)).astype(np.float32)\n",
    "\n",
    "print(\"number of element in each class:\", sum(ynew))\n",
    "\n",
    "# del X\n",
    "# del ylabels\n",
    "\n",
    "\n",
    "# X_all_new = (X_all.T/X_all.sum(axis=1)).T * 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 0 for maximum parallization\n",
    "parallelization_factor = 15\n",
    "\n",
    "\n",
    "LOGDIR = \"./log\"\n",
    "\n",
    "layer_sizes = [Xnew.shape[1], 2000, 1000, 500, 250, 10,2] # X.shape[1]\n",
    "print('layer_sizes', layer_sizes)\n",
    "\n",
    "#TODO X.shape[1]\n",
    "\n",
    "L = len(layer_sizes) - 1  # number of layers\n",
    "\n",
    "num_epochs = 100 #100 # TODO change\n",
    "# num_labeled = 6 #\n",
    "num_examples =  Xnew.shape[0]*6//10  # TODO read from input\n",
    "# tot_number_examples = X.shape[0]\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "# learning_rate = 0.005\n",
    "\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "num_iter = (num_examples//batch_size + 1) * num_epochs  # number of loop iterations\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, shape=(None, layer_sizes[0]), name= \"input\")\n",
    "outputs = tf.placeholder(tf.float32, name = \"output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# training util functions\n",
    "def bi(inits, size, name):\n",
    "    with tf.name_scope(name):\n",
    "        b = tf.Variable(inits * tf.ones([size]), name=\"B\")\n",
    "        tf.summary.histogram(\"bias\", b)\n",
    "        return b\n",
    "\n",
    "def wi(shape, name):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.random_normal(shape, name=\"W\")) / math.sqrt(shape[0])\n",
    "        tf.summary.histogram(\"weight\", w)\n",
    "        print(w)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#training params\n",
    "shapes = list(zip(list(layer_sizes)[:-1], list(layer_sizes[1:])))  # shapes of linear layers\n",
    "print('shapes', shapes)\n",
    "\n",
    "weights = {'W': [wi(s, \"W\") for s in shapes],  # Encoder weights\n",
    "           'V': [wi(s[::-1], \"V\") for s in shapes],  # Decoder weights\n",
    "           # batch normalization parameter to shift the normalized value\n",
    "           'beta': [bi(0.0, layer_sizes[l+1], \"beta\") for l in range(L)],\n",
    "           # batch normalization parameter to scale the normalized value\n",
    "           'gamma': [bi(1.0, layer_sizes[l+1], \"beta\") for l in range(L)]}\n",
    "\n",
    "print(weights['V'],shapes)\n",
    "\n",
    "noise_std = 0.3  # scaling factor for noise used in corrupted encoder\n",
    "\n",
    "# hyperparameters that denote the importance of each layer\n",
    "denoising_cost = [1000.0, 10.0, 0.10, 0.10, 0.10, 0.10, 0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#training params and placeholders\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "ewma = tf.train.ExponentialMovingAverage(decay=0.99)  # to calculate the moving averages of mean and variance\n",
    "bn_assigns = []  # this list stores the updates to be made to average mean and variance\n",
    "\n",
    "\n",
    "def batch_normalization(batch, mean=None, var=None):\n",
    "    if mean is None or var is None:\n",
    "        mean, var = tf.nn.moments(batch, axes=[0])\n",
    "    print(\"batch.shape\", batch.shape)\n",
    "    return (batch - mean) / tf.sqrt(var + tf.constant(1e-10))\n",
    "\n",
    "# average mean and variance of all layers\n",
    "running_mean = [tf.Variable(tf.constant(0.0, shape=[l]), trainable=False) for l in layer_sizes[1:]]\n",
    "running_var = [tf.Variable(tf.constant(1.0, shape=[l]), trainable=False) for l in layer_sizes[1:]]\n",
    "\n",
    "def update_batch_normalization(batch, l):\n",
    "    \"batch normalize + update average mean and variance of layer l\"\n",
    "    mean, var = tf.nn.moments(batch, axes=[0])\n",
    "    assign_mean = running_mean[l-1].assign(mean)\n",
    "    assign_var = running_var[l-1].assign(var)\n",
    "    bn_assigns.append(ewma.apply([running_mean[l-1], running_var[l-1]]))\n",
    "    with tf.control_dependencies([assign_mean, assign_var]):\n",
    "        return (batch - mean) / tf.sqrt(var + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#encoder\n",
    "def encoder(inputs, noise_std):\n",
    "    h = inputs + tf.random_normal(tf.shape(inputs)) * noise_std  # add noise to input\n",
    "    d = {}  # to store the pre-activation, activation, mean and variance for each layer\n",
    "    # The data for labeled and unlabeled examples are stored separately\n",
    "    d['labeled'] = {'z': {}, 'm': {}, 'v': {}, 'h': {}}\n",
    "    d['unlabeled'] = {'z': {}, 'm': {}, 'v': {}, 'h': {}}\n",
    "    d['labeled']['z'][0], d['unlabeled']['z'][0] = split_lu(h)\n",
    "    for l in range(1, L+1):\n",
    "        print (\"Layer \", l, \": \", layer_sizes[l-1], \" -> \", layer_sizes[l])\n",
    "        d['labeled']['h'][l-1], d['unlabeled']['h'][l-1] = split_lu(h)\n",
    "        z_pre = tf.matmul(h, weights['W'][l-1])  # pre-activation\n",
    "        z_pre_l, z_pre_u = split_lu(z_pre)  # split labeled and unlabeled examples\n",
    "\n",
    "        m, v = tf.nn.moments(z_pre_u, axes=[0])\n",
    "\n",
    "        # if training:\n",
    "        def training_batch_norm():\n",
    "            # Training batch normalization\n",
    "            # batch normalization for labeled and unlabeled examples is performed separately\n",
    "            if noise_std > 0:\n",
    "                # Corrupted encoder\n",
    "                # batch normalization + noise\n",
    "                z = join(batch_normalization(z_pre_l), batch_normalization(z_pre_u, m, v))\n",
    "                z += tf.random_normal(tf.shape(z_pre)) * noise_std\n",
    "            else:\n",
    "                # Clean encoder\n",
    "                # batch normalization + update the average mean and variance using batch mean and variance of labeled examples\n",
    "                z = join(update_batch_normalization(z_pre_l, l), batch_normalization(z_pre_u, m, v))\n",
    "            return z\n",
    "\n",
    "        # else:\n",
    "        def eval_batch_norm():\n",
    "            # Evaluation batch normalization\n",
    "            # obtain average mean and variance and use it to normalize the batch\n",
    "            mean = ewma.average(running_mean[l-1])\n",
    "            var = ewma.average(running_var[l-1])\n",
    "            z = batch_normalization(z_pre, mean, var)\n",
    "            # Instead of the above statement, the use of the following 2 statements containing a typo\n",
    "            # consistently produces a 0.2% higher accuracy for unclear reasons.\n",
    "            return z\n",
    "\n",
    "        # perform batch normalization according to value of boolean \"training\" placeholder:\n",
    "        z = tf.cond(training, training_batch_norm, eval_batch_norm)\n",
    "\n",
    "        if l == L:\n",
    "            # use softmax activation in output layer\n",
    "            h = tf.nn.softmax(weights['gamma'][l-1] * (z + weights[\"beta\"][l-1]))\n",
    "        else:\n",
    "            # use ReLU activation in hidden layers\n",
    "            h = tf.nn.relu(z + weights[\"beta\"][l-1])\n",
    "        d['labeled']['z'][l], d['unlabeled']['z'][l] = split_lu(z)\n",
    "        d['unlabeled']['m'][l], d['unlabeled']['v'][l] = m, v  # save mean and variance of unlabeled examples for decoding\n",
    "    d['labeled']['h'][l], d['unlabeled']['h'][l] = split_lu(h)\n",
    "    return h, d\n",
    "print (\"=== Corrupted Encoder ===\")\n",
    "y_c, corr = encoder(inputs, noise_std)\n",
    "\n",
    "print (\"=== Clean Encoder ===\")\n",
    "y, clean = encoder(inputs, 0.0)  # 0.0 -> do not add noise\n",
    "\n",
    "print (\"=== Decoder ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def g_gauss(z_c, u, size):\n",
    "    \"gaussian denoising function proposed in the original paper\"\n",
    "    wi = lambda inits, name: tf.Variable(inits * tf.ones([size]), name=name)\n",
    "    a1 = wi(0., 'a1')\n",
    "    a2 = wi(1., 'a2')\n",
    "    a3 = wi(0., 'a3')\n",
    "    a4 = wi(0., 'a4')\n",
    "    a5 = wi(0., 'a5')\n",
    "\n",
    "    a6 = wi(0., 'a6')\n",
    "    a7 = wi(1., 'a7')\n",
    "    a8 = wi(0., 'a8')\n",
    "    a9 = wi(0., 'a9')\n",
    "    a10 = wi(0., 'a10')\n",
    "\n",
    "    mu = a1 * tf.sigmoid(a2 * u + a3) + a4 * u + a5\n",
    "    v = a6 * tf.sigmoid(a7 * u + a8) + a9 * u + a10\n",
    "\n",
    "    z_est = (z_c - mu) * v + mu\n",
    "    return z_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "z_est = {}\n",
    "d_cost = []  # to store the denoising cost of all layers\n",
    "for l in range(L, -1, -1):\n",
    "    print (\"Layer \", l, \": \", layer_sizes[l+1] if l+1 < len(layer_sizes) else None, \" -> \", layer_sizes[l], \", denoising cost: \", denoising_cost[l])\n",
    "    z, z_c = clean['unlabeled']['z'][l], corr['unlabeled']['z'][l]\n",
    "    m, v = clean['unlabeled']['m'].get(l, 0), clean['unlabeled']['v'].get(l, 1-1e-10)\n",
    "    if l == L:\n",
    "        u = unlabeled(y_c)\n",
    "    else:\n",
    "        u = tf.matmul(z_est[l+1], weights['V'][l])\n",
    "    u = batch_normalization(u)\n",
    "    z_est[l] = g_gauss(z_c, u, layer_sizes[l])\n",
    "    z_est_bn = (z_est[l] - m) / v\n",
    "    # append the cost of this layer to d_cost\n",
    "    d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z_est_bn - z), 1)) / layer_sizes[l]) * denoising_cost[l])\n",
    "\n",
    "# calculate total unsupervised cost by adding the denoising cost of all layers\n",
    "u_cost = tf.add_n(d_cost)\n",
    "\n",
    "y_N = labeled(y_c)\n",
    "cost = -tf.reduce_mean(tf.reduce_sum(outputs*tf.log(y_N), 1))  # supervised cost\n",
    "loss = cost + u_cost  # total cost\n",
    "\n",
    "pred_cost = -tf.reduce_mean(tf.reduce_sum(outputs*tf.log(y), 1))  # cost used for prediction\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(outputs, 1))  # no of correct predictions\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) * tf.constant(100.0)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "#learning_rate = tf.Variable(starter_learning_rate, trainable=False)\n",
    "with tf.name_scope(\"train\"):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "\n",
    "\n",
    "# add the updates of batch normalization statistics to train_step\n",
    "bn_updates = tf.group(*bn_assigns)\n",
    "with tf.control_dependencies([train_step]):\n",
    "    train_step = tf.group(bn_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create checkpoint \n",
    "#     ckpt = tf.train.get_checkpoint_state('checkpoints_cancer/')  # get latest checkpoint (if any)\n",
    "#     if ckpt and ckpt.model_checkpoint_path:\n",
    "#         # if checkpoint exists, restore the parameters and set epoch_n and i_iter\n",
    "#         saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "#         epoch_n = int(ckpt.model_checkpoint_path.split('-')[1])\n",
    "#         i_iter = (epoch_n+1) * (num_examples//batch_size)\n",
    "#         print (\"Restored Epoch \", epoch_n)\n",
    "#     else:\n",
    "#         # no checkpoint exists. create checkpoints directory if it does not exist.\n",
    "#         if not os.path.exists('checkpoints_cancer'):\n",
    "#             os.makedirs('checkpoints_cancer')\n",
    "#         writer = tf.summary.FileWriter('./log', sess.graph)\n",
    "#         init = tf.global_variables_initializer()\n",
    "#         sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_accuracies(epoch, sess, datasets) :\n",
    "#     y_true = np.argmax(datasets.test.labels,1)\n",
    "    \n",
    "    \n",
    "#     print(datasets.train.labeled_ds.dataset.shape,  datasets.train.labeled_ds.labels.shape)\n",
    "\n",
    "#     print(datasets.validation.dataset.shape,  datasets.validation.labels.shape)\n",
    "\n",
    "    train_acc = sess.run(accuracy, feed_dict={inputs: datasets.train.unlabeled_ds.dataset, outputs: datasets.train.unlabeled_ds.labels, training: False})\n",
    "    validation_acc = sess.run(accuracy, feed_dict={inputs: datasets.validation.dataset, outputs: datasets.validation.labels, training: False})\n",
    "    \n",
    "#     y_p = tf.argmax(y, 1)\n",
    "#     test_acc, y_predicted = sess.run([accuracy,y_p], feed_dict={inputs: datasets.test.dataset, outputs: datasets.test.labels, training: False})\n",
    "#     test_f1 = str(sk.metrics.f1_score(y_true, y_predicted))\n",
    "    \n",
    "    print(epoch, \" =>\", \" train: \", train_acc, \" validation: \", validation_acc #, \" test: \", test_acc, \" f1(test): \" + test_f1\n",
    "         )\n",
    "    return train_acc, validation_acc, '' # test_acc\n",
    "\n",
    "sess = _\n",
    "\n",
    "def run_model(datasets):\n",
    "    global sess\n",
    "    expression_dataset = datasets\n",
    "\n",
    "    saver = tf.train.Saver(write_version=tf.train.SaverDef.V1)\n",
    "\n",
    "    sess = tf.Session(config=\n",
    "        tf.ConfigProto(inter_op_parallelism_threads=parallelization_factor,\n",
    "                   intra_op_parallelism_threads=parallelization_factor))\n",
    "    \n",
    "    i_iter = 0\n",
    "\n",
    "    \n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    acc_count = 0\n",
    "    \n",
    "#     train_acc = sess.run(accuracy, feed_dict={inputs: expression_dataset.train.unlabeled_ds.dataset, outputs: expression_dataset.train.unlabeled_ds.labels, training: False})\n",
    "#     validation_acc = sess.run(accuracy, feed_dict={inputs: expression_dataset.validation.dataset, outputs: expression_dataset.validation.labels, training: False})\n",
    "#     test_acc = sess.run(accuracy, feed_dict={inputs: expression_dataset.test.dataset, outputs: expression_dataset.test.labels, training: False})\n",
    "    \n",
    "    \n",
    "#     print(\"INITIAL VALUES\")\n",
    "    _, pre_acc, _ = get_accuracies(\"Initial\", sess, expression_dataset)\n",
    "    \n",
    "    \n",
    "#     print('i_iter: ', i_iter, ' num_iter:', num_iter)\n",
    "#     print('num_examples', num_examples)\n",
    "\n",
    "#     for i in tqdm(range(i_iter, num_iter)):\n",
    "    for i in (range(i_iter, num_iter)):\n",
    "\n",
    "        dataset, labels = expression_dataset.train.next_batch(batch_size)\n",
    "#         print(dataset.shape)\n",
    "#         print(labels.shape)\n",
    "\n",
    "        sess.run(train_step, feed_dict={inputs: dataset, outputs: labels, training: True})\n",
    "\n",
    "        if (i > 1) and ((i+1) % (num_iter//num_epochs) == 0):\n",
    "            epoch_n = i//(num_examples//batch_size)\n",
    "            \n",
    "            _, curr_acc, _ = get_accuracies(\"Epoch(\" + str(epoch_n) + \")\", sess, expression_dataset)\n",
    "            \n",
    "            if curr_acc <= pre_acc*1.0001 and curr_acc/pre_acc > 0.95 :\n",
    "                acc_count += 1\n",
    "            else :\n",
    "                acc_count = 0\n",
    "                pre_acc = curr_acc\n",
    "            print(pre_acc,acc_count )\n",
    "            patience = 20\n",
    "                \n",
    "            # TODO EARLY STOPPING\n",
    "            if acc_count > patience:\n",
    "                print(\"Early stop!!!!!\", acc_count, epoch_n)\n",
    "                break\n",
    "\n",
    "    get_accuracies(\"FINAL\", sess, expression_dataset)\n",
    "\n",
    "    y_p = tf.argmax(y, 1)\n",
    "    y_pred = sess.run(y_p, feed_dict={inputs: expression_dataset.test.dataset, training: False})\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     print (\"TEST accuracy:\", test_accuracy)\n",
    "    y_true = np.argmax(expression_dataset.test.labels,1)\n",
    "    print (\"Precision\", sk.metrics.precision_score(y_true, y_pred))\n",
    "    print (\"Recall\", sk.metrics.recall_score(y_true, y_pred))\n",
    "    print (\"f1_score\", sk.metrics.f1_score(y_true, y_pred))\n",
    "    print (\"confusion_matrix\")\n",
    "    print (sk.metrics.confusion_matrix(y_true, y_pred))\n",
    "    with open(file, \"a\") as text_file:\n",
    "        text_file.write(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % ('', \n",
    "                                                sk.metrics.accuracy_score(y_true, y_pred), \n",
    "                                                sk.metrics.f1_score(y_true, y_pred), \n",
    "                                                sk.metrics.precision_score(y_true, y_pred), \n",
    "                                                sk.metrics.recall_score(y_true, y_pred),\n",
    "                                                sk.metrics.confusion_matrix(y_true, y_pred).tolist()))\n",
    "\n",
    "\n",
    "    sess.close()\n",
    "#     raise ValueError('A very specific bad thing happened.')\n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "skf = sklearn.model_selection.RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=123)\n",
    "\n",
    "with open(file, \"w\") as text_file:\n",
    "    text_file.write(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % ('fold','accuracy', 'f1-score', 'precision', 'recall', 'conf_m'))\n",
    "    \n",
    "all_y_true = np.array([]).astype(int)\n",
    "all_y_pred = np.array([]).astype(int)\n",
    "\n",
    "print(Xnew.shape)\n",
    "\n",
    "\n",
    "# print('XXXXX',    len(Xnew), len(ynew))\n",
    "\n",
    "for train_valid_index, test_index in skf.split(Xnew, ynew[:,0]):\n",
    "    \n",
    "#     print('XXXXX', len(train_valid_index), len(test_index))\n",
    "\n",
    "\n",
    "    X_train_valid,  X_test = Xnew[train_valid_index], Xnew[test_index]\n",
    "    y_train_valid,  y_test = ynew[train_valid_index], ynew[test_index]\n",
    "    \n",
    "        \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train_valid)\n",
    "    X_train_valid = scaler.transform(X_train_valid)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid= train_test_split(X_train_valid, y_train_valid, test_size=0.25, stratify=y_train_valid[:,0])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#     from imblearn.over_sampling import RandomOverSampler \n",
    "#     ros = RandomOverSampler(random_state=0)\n",
    "#     X_train, y_train = ros.fit_sample(X_train, y_train[:,0])\n",
    "#     y_train = np.array(list(zip(y_train, 1- y_train)))\n",
    "    \n",
    "\n",
    "#     X_valid, y_valid = ros.fit_sample(X_valid, y_valid[:,0])\n",
    "#     y_valid = np.array(list(zip(y_valid, 1- y_valid)))\n",
    "    \n",
    "                         \n",
    "#     print('YYYYY', len(X_train), len(X_valid), len(X_test))\n",
    "#     print('ZZZZZ', len(y_train), len(y_valid), len(y_test))\n",
    "#     print('QQQQQ', sum(y_train[:,0]) / len(y_train), sum(y_valid[:,0]) / len(y_valid), sum(y_test[:,0]) / len(y_test))\n",
    "\n",
    "\n",
    "#     print(X_train.shape)\n",
    "    \n",
    "    data_sets.train = SemiDataSet(X_train,y_train , 60)\n",
    "\n",
    "#     data_sets.train = SemiDataSet2(X_train,y_train , X_all)\n",
    "\n",
    "    data_sets.validation = DataSet(X_valid,y_valid)\n",
    "    data_sets.test = DataSet(X_test,y_test)\n",
    "    \n",
    "    y_true, y_pred = run_model(data_sets)\n",
    "    \n",
    "\n",
    "    all_y_true = np.append(all_y_true,y_true)\n",
    "    all_y_pred = np.append(all_y_pred,y_pred)\n",
    "    \n",
    "\n",
    "\n",
    "#     break\n",
    "print (sk.metrics.confusion_matrix(all_y_true, all_y_pred))\n",
    "with open(file, \"a\") as text_file:\n",
    "    text_file.write(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % ('ALL', \n",
    "                        sk.metrics.accuracy_score(all_y_true, all_y_pred), \n",
    "                        sk.metrics.f1_score(all_y_true, all_y_pred), \n",
    "                        sk.metrics.precision_score(all_y_true, all_y_pred), \n",
    "                        sk.metrics.recall_score(all_y_true, all_y_pred),\n",
    "                        sk.metrics.confusion_matrix(all_y_true, all_y_pred).tolist())\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
