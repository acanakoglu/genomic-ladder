{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#This is the modified version of the ladder network code from https://github.com/rinuboney/ladder\n",
    "#Certain modfications are made to use & experiment with gene expression data\n",
    "#\n",
    "# import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "#import input_data\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "join = lambda l, u: tf.concat([l, u], 0)\n",
    "labeled = lambda x: tf.slice(x, [0, 0], [batch_size, -1]) if x is not None else x\n",
    "unlabeled = lambda x: tf.slice(x, [batch_size, 0], [-1, -1]) if x is not None else x\n",
    "split_lu = lambda x: (labeled(x), unlabeled(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out/BRCA.tsv'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cancer_type = 'aggregates/LUNG'\n",
    "cancer_type = 'BRCA'\n",
    "\n",
    "file = 'out/' + cancer_type.replace(\"/\",\"_\") +  \".tsv\"\n",
    "file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     2,
     49
    ]
   },
   "outputs": [],
   "source": [
    "#class definitions\n",
    "\n",
    "class DataSet(object):\n",
    "\n",
    "  def __init__(self, dataset, labels):\n",
    "    \n",
    "    self._dataset = dataset\n",
    "    self._labels = labels\n",
    "    self._epochs_completed = 0\n",
    "    self._index_in_epoch = 0\n",
    "    self._num_examples = dataset.shape[0]\n",
    "\n",
    "  @property\n",
    "  def dataset(self):\n",
    "    return self._dataset\n",
    "\n",
    "  @property\n",
    "  def labels(self):\n",
    "    return self._labels\n",
    "\n",
    "  @property\n",
    "  def num_examples(self):\n",
    "    return self._num_examples\n",
    "\n",
    "  @property\n",
    "  def epochs_completed(self):\n",
    "    return self._epochs_completed\n",
    "\n",
    "  def next_batch(self, batch_size):\n",
    "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "#     print (\"index\", self._index_in_epoch)\n",
    "    start = self._index_in_epoch\n",
    "    self._index_in_epoch += batch_size\n",
    "    if self._index_in_epoch > self._num_examples:\n",
    "        # Finished epoch\n",
    "        self._epochs_completed += 1\n",
    "        # Shuffle the data\n",
    "        perm = np.arange(self._num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        self._dataset = self._dataset[perm]\n",
    "        self._labels = self._labels[perm]\n",
    "        # Start next epoch\n",
    "        start = 0\n",
    "        self._index_in_epoch = batch_size\n",
    "#         print(batch_size, self._num_examples)\n",
    "        assert batch_size <= self._num_examples\n",
    "    end = self._index_in_epoch\n",
    "    return self._dataset[start:end], self._labels[start:end]\n",
    "\n",
    "class SemiDataSet(object):\n",
    "    def __init__(self, dataset, labels, n_labeled):\n",
    "        \n",
    "        self.n_labeled = n_labeled\n",
    "\n",
    "        # Unlabled DataSet\n",
    "        self.unlabeled_ds = DataSet(dataset, labels)\n",
    "\n",
    "        # Labeled DataSet\n",
    "        self.num_examples = self.unlabeled_ds.num_examples\n",
    "        indices = np.arange(self.num_examples)\n",
    "        shuffled_indices = np.random.permutation(indices)\n",
    "        dataset = dataset[shuffled_indices]\n",
    "        labels = labels[shuffled_indices]\n",
    "#         print('labels',labels)\n",
    "        \n",
    "        y = np.array([np.arange(2)[l==1][0] for l in labels])\n",
    "#         print('y',y)\n",
    "#         global test\n",
    "#         test=labels\n",
    "\n",
    "        \n",
    "#         idx = indices[y==0][:5]\n",
    "#         print('idx',idx)\n",
    "\n",
    "\n",
    "        n_classes = y.max() + 1\n",
    "        print('n_classes',n_classes)\n",
    "        n_from_each_class = n_labeled // n_classes\n",
    "        i_labeled = []\n",
    "        for c in range(n_classes):\n",
    "            i = indices[y==c][:n_from_each_class]\n",
    "            i_labeled += list(i)\n",
    "        l_dataset = dataset[i_labeled]\n",
    "        l_labels = labels[i_labeled]\n",
    "        self.labeled_ds = DataSet(l_dataset, l_labels)\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        #print (\"batch size semi\", batch_size)\n",
    "        unlabeled_dataset, _ = self.unlabeled_ds.next_batch(batch_size)\n",
    "     \n",
    "        if batch_size > self.n_labeled:\n",
    "            labeled_dataset, labels = self.labeled_ds.next_batch(self.n_labeled)\n",
    "        else:\n",
    "            labeled_dataset, labels = self.labeled_ds.next_batch(batch_size)\n",
    "            #print (labeled_dataset.shape)\n",
    "        #print (\"labels shape aasd\", labels.shape)\n",
    "        #print (labels)\n",
    "        dataset = np.vstack([labeled_dataset, unlabeled_dataset])\n",
    "        return dataset, labels\n",
    "# aa = SemiDataSet(X_train,y_train , 60)\n",
    "# aa.next_batch(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#one-hot label\n",
    "def dense_to_one_hot(labels_dense, num_classes=2):\n",
    "\n",
    "  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "  num_labels = labels_dense.shape[0]\n",
    "#   print(num_labels)\n",
    "  index_offset = np.arange(num_labels) * num_classes\n",
    "  labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "  return labels_one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#fix labels  1 for tumoral, 0 for healthy\n",
    "def fix_label(labels):\n",
    "    labels= [1 if x==1 else 0 for x in labels]\n",
    "    \n",
    "    return np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1218, 20530)\n",
      "(1218,)\n"
     ]
    }
   ],
   "source": [
    "wd = '../nanni_data_tcga_cibb/'+cancer_type+'/'\n",
    "# wd = '/home/nanni/Data/TCGA/CIBB/aggregates/LUNG/'\n",
    "# wd = '/home/nanni/Data/TCGA/CIBB/aggregates/KIDNEY/'\n",
    "\n",
    "\n",
    "X_file = wd + '/X.npy'\n",
    "y_file = wd + '/y.npy'\n",
    "X = np.load(X_file)\n",
    "y = np.load(y_file)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===  Loading Data ===\n",
      "(1218, 20530)\n",
      "(1218,)\n",
      "# of 1s 114.0\n",
      "number of element in each class: [1104.  114.]\n"
     ]
    }
   ],
   "source": [
    "print (\"===  Loading Data ===\")\n",
    "\n",
    "\n",
    "class DataSets(object):\n",
    "    pass\n",
    "data_sets = DataSets()\n",
    "\n",
    "\n",
    "X = np.load(X_file)\n",
    "ylabels = np.load(y_file)\n",
    "print(X.shape)\n",
    "print(ylabels.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO remove all zeros\n",
    "scaler = MinMaxScaler()\n",
    "# Xnew = scaler.fit_transform(X.T).T\n",
    "\n",
    "# Xnew = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Xnew = scaler.fit_transform(X)\n",
    "\n",
    "# Xnew = (X.T/X.sum(axis=1)).T * 10000\n",
    "\n",
    "\n",
    "\n",
    "# Xnew = X\n",
    "\n",
    "\n",
    "Xnew = X.T[X.sum(axis =0) != 0].T\n",
    "\n",
    "print('# of 1s', sum(ylabels))\n",
    "\n",
    "ynew = dense_to_one_hot(fix_label(ylabels)).astype(np.float32)\n",
    "\n",
    "print(\"number of element in each class:\", sum(ynew))\n",
    "\n",
    "# del X\n",
    "# del ylabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_sizes [20252, 2000, 1000, 500, 250, 10, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 0 for maximum parallization\n",
    "parallelization_factor = 15\n",
    "\n",
    "\n",
    "LOGDIR = \"./log\"\n",
    "\n",
    "layer_sizes = [Xnew.shape[1], 2000, 1000, 500, 250, 10,2] # X.shape[1]\n",
    "print('layer_sizes', layer_sizes)\n",
    "\n",
    "#TODO X.shape[1]\n",
    "\n",
    "L = len(layer_sizes) - 1  # number of layers\n",
    "\n",
    "num_epochs = 100 #100 # TODO change\n",
    "# num_labeled = 6 #\n",
    "num_examples = X.shape[0] # TODO read from input\n",
    "# tot_number_examples = X.shape[0]\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "num_iter = (num_examples//batch_size + 1) * num_epochs  # number of loop iterations\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, shape=(None, layer_sizes[0]), name= \"input\")\n",
    "outputs = tf.placeholder(tf.float32, name = \"output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#training util functions\n",
    "def bi(inits, size, name):\n",
    "    with tf.name_scope(name):\n",
    "        b = tf.Variable(inits * tf.ones([size]), name=\"B\")\n",
    "        tf.summary.histogram(\"bias\", b)\n",
    "        return b\n",
    "\n",
    "def wi(shape, name):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.random_normal(shape, name=\"W\")) / math.sqrt(shape[0])\n",
    "        tf.summary.histogram(\"weight\", w)\n",
    "        print(w)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes [(20252, 2000), (2000, 1000), (1000, 500), (500, 250), (250, 10), (10, 2)]\n",
      "Tensor(\"W/truediv:0\", shape=(20252, 2000), dtype=float32)\n",
      "Tensor(\"W_1/truediv:0\", shape=(2000, 1000), dtype=float32)\n",
      "Tensor(\"W_2/truediv:0\", shape=(1000, 500), dtype=float32)\n",
      "Tensor(\"W_3/truediv:0\", shape=(500, 250), dtype=float32)\n",
      "Tensor(\"W_4/truediv:0\", shape=(250, 10), dtype=float32)\n",
      "Tensor(\"W_5/truediv:0\", shape=(10, 2), dtype=float32)\n",
      "Tensor(\"V/truediv:0\", shape=(2000, 20252), dtype=float32)\n",
      "Tensor(\"V_1/truediv:0\", shape=(1000, 2000), dtype=float32)\n",
      "Tensor(\"V_2/truediv:0\", shape=(500, 1000), dtype=float32)\n",
      "Tensor(\"V_3/truediv:0\", shape=(250, 500), dtype=float32)\n",
      "Tensor(\"V_4/truediv:0\", shape=(10, 250), dtype=float32)\n",
      "Tensor(\"V_5/truediv:0\", shape=(2, 10), dtype=float32)\n",
      "[<tf.Tensor 'V/truediv:0' shape=(2000, 20252) dtype=float32>, <tf.Tensor 'V_1/truediv:0' shape=(1000, 2000) dtype=float32>, <tf.Tensor 'V_2/truediv:0' shape=(500, 1000) dtype=float32>, <tf.Tensor 'V_3/truediv:0' shape=(250, 500) dtype=float32>, <tf.Tensor 'V_4/truediv:0' shape=(10, 250) dtype=float32>, <tf.Tensor 'V_5/truediv:0' shape=(2, 10) dtype=float32>] [(20252, 2000), (2000, 1000), (1000, 500), (500, 250), (250, 10), (10, 2)]\n"
     ]
    }
   ],
   "source": [
    "#training params\n",
    "shapes = list(zip(list(layer_sizes)[:-1], list(layer_sizes[1:])))  # shapes of linear layers\n",
    "print('shapes', shapes)\n",
    "\n",
    "weights = {'W': [wi(s, \"W\") for s in shapes],  # Encoder weights\n",
    "           'V': [wi(s[::-1], \"V\") for s in shapes],  # Decoder weights\n",
    "           # batch normalization parameter to shift the normalized value\n",
    "           'beta': [bi(0.0, layer_sizes[l+1], \"beta\") for l in range(L)],\n",
    "           # batch normalization parameter to scale the normalized value\n",
    "           'gamma': [bi(1.0, layer_sizes[l+1], \"beta\") for l in range(L)]}\n",
    "\n",
    "print(weights['V'],shapes)\n",
    "\n",
    "noise_std = 0.3  # scaling factor for noise used in corrupted encoder\n",
    "\n",
    "# hyperparameters that denote the importance of each layer\n",
    "denoising_cost = [1000.0, 10.0, 0.10, 0.10, 0.10, 0.10, 0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#training params and placeholders\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "ewma = tf.train.ExponentialMovingAverage(decay=0.99)  # to calculate the moving averages of mean and variance\n",
    "bn_assigns = []  # this list stores the updates to be made to average mean and variance\n",
    "\n",
    "\n",
    "def batch_normalization(batch, mean=None, var=None):\n",
    "    if mean is None or var is None:\n",
    "        mean, var = tf.nn.moments(batch, axes=[0])\n",
    "    return (batch - mean) / tf.sqrt(var + tf.constant(1e-10))\n",
    "\n",
    "# average mean and variance of all layers\n",
    "running_mean = [tf.Variable(tf.constant(0.0, shape=[l]), trainable=False) for l in layer_sizes[1:]]\n",
    "running_var = [tf.Variable(tf.constant(1.0, shape=[l]), trainable=False) for l in layer_sizes[1:]]\n",
    "\n",
    "def update_batch_normalization(batch, l):\n",
    "    \"batch normalize + update average mean and variance of layer l\"\n",
    "    mean, var = tf.nn.moments(batch, axes=[0])\n",
    "    assign_mean = running_mean[l-1].assign(mean)\n",
    "    assign_var = running_var[l-1].assign(var)\n",
    "    bn_assigns.append(ewma.apply([running_mean[l-1], running_var[l-1]]))\n",
    "    with tf.control_dependencies([assign_mean, assign_var]):\n",
    "        return (batch - mean) / tf.sqrt(var + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Corrupted Encoder ===\n",
      "Layer  1 :  20252  ->  2000\n",
      "Layer  2 :  2000  ->  1000\n",
      "Layer  3 :  1000  ->  500\n",
      "Layer  4 :  500  ->  250\n",
      "Layer  5 :  250  ->  10\n",
      "Layer  6 :  10  ->  2\n",
      "=== Clean Encoder ===\n",
      "Layer  1 :  20252  ->  2000\n",
      "Layer  2 :  2000  ->  1000\n",
      "Layer  3 :  1000  ->  500\n",
      "Layer  4 :  500  ->  250\n",
      "Layer  5 :  250  ->  10\n",
      "Layer  6 :  10  ->  2\n",
      "=== Decoder ===\n"
     ]
    }
   ],
   "source": [
    "#encoder\n",
    "def encoder(inputs, noise_std):\n",
    "    h = inputs + tf.random_normal(tf.shape(inputs)) * noise_std  # add noise to input\n",
    "    d = {}  # to store the pre-activation, activation, mean and variance for each layer\n",
    "    # The data for labeled and unlabeled examples are stored separately\n",
    "    d['labeled'] = {'z': {}, 'm': {}, 'v': {}, 'h': {}}\n",
    "    d['unlabeled'] = {'z': {}, 'm': {}, 'v': {}, 'h': {}}\n",
    "    d['labeled']['z'][0], d['unlabeled']['z'][0] = split_lu(h)\n",
    "    for l in range(1, L+1):\n",
    "        print (\"Layer \", l, \": \", layer_sizes[l-1], \" -> \", layer_sizes[l])\n",
    "        d['labeled']['h'][l-1], d['unlabeled']['h'][l-1] = split_lu(h)\n",
    "        z_pre = tf.matmul(h, weights['W'][l-1])  # pre-activation\n",
    "        z_pre_l, z_pre_u = split_lu(z_pre)  # split labeled and unlabeled examples\n",
    "\n",
    "        m, v = tf.nn.moments(z_pre_u, axes=[0])\n",
    "\n",
    "        # if training:\n",
    "        def training_batch_norm():\n",
    "            # Training batch normalization\n",
    "            # batch normalization for labeled and unlabeled examples is performed separately\n",
    "            if noise_std > 0:\n",
    "                # Corrupted encoder\n",
    "                # batch normalization + noise\n",
    "                z = join(batch_normalization(z_pre_l), batch_normalization(z_pre_u, m, v))\n",
    "                z += tf.random_normal(tf.shape(z_pre)) * noise_std\n",
    "            else:\n",
    "                # Clean encoder\n",
    "                # batch normalization + update the average mean and variance using batch mean and variance of labeled examples\n",
    "                z = join(update_batch_normalization(z_pre_l, l), batch_normalization(z_pre_u, m, v))\n",
    "            return z\n",
    "\n",
    "        # else:\n",
    "        def eval_batch_norm():\n",
    "            # Evaluation batch normalization\n",
    "            # obtain average mean and variance and use it to normalize the batch\n",
    "            mean = ewma.average(running_mean[l-1])\n",
    "            var = ewma.average(running_var[l-1])\n",
    "            z = batch_normalization(z_pre, mean, var)\n",
    "            # Instead of the above statement, the use of the following 2 statements containing a typo\n",
    "            # consistently produces a 0.2% higher accuracy for unclear reasons.\n",
    "            return z\n",
    "\n",
    "        # perform batch normalization according to value of boolean \"training\" placeholder:\n",
    "        z = tf.cond(training, training_batch_norm, eval_batch_norm)\n",
    "\n",
    "        if l == L:\n",
    "            # use softmax activation in output layer\n",
    "            h = tf.nn.softmax(weights['gamma'][l-1] * (z + weights[\"beta\"][l-1]))\n",
    "        else:\n",
    "            # use ReLU activation in hidden layers\n",
    "            h = tf.nn.relu(z + weights[\"beta\"][l-1])\n",
    "        d['labeled']['z'][l], d['unlabeled']['z'][l] = split_lu(z)\n",
    "        d['unlabeled']['m'][l], d['unlabeled']['v'][l] = m, v  # save mean and variance of unlabeled examples for decoding\n",
    "    d['labeled']['h'][l], d['unlabeled']['h'][l] = split_lu(h)\n",
    "    return h, d\n",
    "print (\"=== Corrupted Encoder ===\")\n",
    "y_c, corr = encoder(inputs, noise_std)\n",
    "\n",
    "print (\"=== Clean Encoder ===\")\n",
    "y, clean = encoder(inputs, 0.0)  # 0.0 -> do not add noise\n",
    "\n",
    "print (\"=== Decoder ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def g_gauss(z_c, u, size):\n",
    "    \"gaussian denoising function proposed in the original paper\"\n",
    "    wi = lambda inits, name: tf.Variable(inits * tf.ones([size]), name=name)\n",
    "    a1 = wi(0., 'a1')\n",
    "    a2 = wi(1., 'a2')\n",
    "    a3 = wi(0., 'a3')\n",
    "    a4 = wi(0., 'a4')\n",
    "    a5 = wi(0., 'a5')\n",
    "\n",
    "    a6 = wi(0., 'a6')\n",
    "    a7 = wi(1., 'a7')\n",
    "    a8 = wi(0., 'a8')\n",
    "    a9 = wi(0., 'a9')\n",
    "    a10 = wi(0., 'a10')\n",
    "\n",
    "    mu = a1 * tf.sigmoid(a2 * u + a3) + a4 * u + a5\n",
    "    v = a6 * tf.sigmoid(a7 * u + a8) + a9 * u + a10\n",
    "\n",
    "    z_est = (z_c - mu) * v + mu\n",
    "    return z_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer  6 :  None  ->  2 , denoising cost:  0.1\n",
      "Layer  5 :  2  ->  10 , denoising cost:  0.1\n",
      "Layer  4 :  10  ->  250 , denoising cost:  0.1\n",
      "Layer  3 :  250  ->  500 , denoising cost:  0.1\n",
      "Layer  2 :  500  ->  1000 , denoising cost:  0.1\n",
      "Layer  1 :  1000  ->  2000 , denoising cost:  10.0\n",
      "Layer  0 :  2000  ->  20252 , denoising cost:  1000.0\n"
     ]
    }
   ],
   "source": [
    "# Decoder\n",
    "z_est = {}\n",
    "d_cost = []  # to store the denoising cost of all layers\n",
    "for l in range(L, -1, -1):\n",
    "    print (\"Layer \", l, \": \", layer_sizes[l+1] if l+1 < len(layer_sizes) else None, \" -> \", layer_sizes[l], \", denoising cost: \", denoising_cost[l])\n",
    "    z, z_c = clean['unlabeled']['z'][l], corr['unlabeled']['z'][l]\n",
    "    m, v = clean['unlabeled']['m'].get(l, 0), clean['unlabeled']['v'].get(l, 1-1e-10)\n",
    "    if l == L:\n",
    "        u = unlabeled(y_c)\n",
    "    else:\n",
    "        u = tf.matmul(z_est[l+1], weights['V'][l])\n",
    "    u = batch_normalization(u)\n",
    "    z_est[l] = g_gauss(z_c, u, layer_sizes[l])\n",
    "    z_est_bn = (z_est[l] - m) / v\n",
    "    # append the cost of this layer to d_cost\n",
    "    d_cost.append((tf.reduce_mean(tf.reduce_sum(tf.square(z_est_bn - z), 1)) / layer_sizes[l]) * denoising_cost[l])\n",
    "\n",
    "# calculate total unsupervised cost by adding the denoising cost of all layers\n",
    "u_cost = tf.add_n(d_cost)\n",
    "\n",
    "y_N = labeled(y_c)\n",
    "cost = -tf.reduce_mean(tf.reduce_sum(outputs*tf.log(y_N), 1))  # supervised cost\n",
    "loss = cost + u_cost  # total cost\n",
    "\n",
    "pred_cost = -tf.reduce_mean(tf.reduce_sum(outputs*tf.log(y), 1))  # cost used for prediction\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(outputs, 1))  # no of correct predictions\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) * tf.constant(100.0)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "#learning_rate = tf.Variable(starter_learning_rate, trainable=False)\n",
    "with tf.name_scope(\"train\"):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "\n",
    "\n",
    "# add the updates of batch normalization statistics to train_step\n",
    "bn_updates = tf.group(*bn_assigns)\n",
    "with tf.control_dependencies([train_step]):\n",
    "    train_step = tf.group(bn_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = (X.T/X.sum(axis=1)).T * 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6551339519268424"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #create checkpoint \n",
    "#     ckpt = tf.train.get_checkpoint_state('checkpoints_cancer/')  # get latest checkpoint (if any)\n",
    "#     if ckpt and ckpt.model_checkpoint_path:\n",
    "#         # if checkpoint exists, restore the parameters and set epoch_n and i_iter\n",
    "#         saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "#         epoch_n = int(ckpt.model_checkpoint_path.split('-')[1])\n",
    "#         i_iter = (epoch_n+1) * (num_examples//batch_size)\n",
    "#         print (\"Restored Epoch \", epoch_n)\n",
    "#     else:\n",
    "#         # no checkpoint exists. create checkpoints directory if it does not exist.\n",
    "#         if not os.path.exists('checkpoints_cancer'):\n",
    "#             os.makedirs('checkpoints_cancer')\n",
    "#         writer = tf.summary.FileWriter('./log', sess.graph)\n",
    "#         init = tf.global_variables_initializer()\n",
    "#         sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_accuracies(epoch, sess, datasets) :\n",
    "    y_true = np.argmax(datasets.test.labels,1)\n",
    "    \n",
    "    train_acc = sess.run(accuracy, feed_dict={inputs: datasets.train.unlabeled_ds.dataset, outputs: datasets.train.unlabeled_ds.labels, training: False})\n",
    "    validation_acc = sess.run(accuracy, feed_dict={inputs: datasets.validation.dataset, outputs: datasets.validation.labels, training: False})\n",
    "    \n",
    "    y_p = tf.argmax(y, 1)\n",
    "    test_acc, y_predicted = sess.run([accuracy,y_p], feed_dict={inputs: datasets.test.dataset, outputs: datasets.test.labels, training: False})\n",
    "    test_f1 = str(sk.metrics.f1_score(y_true, y_predicted))\n",
    "    \n",
    "    print(epoch, \" =>\", \" train: \", train_acc, \" validation: \", validation_acc, \" test: \", test_acc, \" f1(test): \" + test_f1)\n",
    "    return train_acc, validation_acc, test_acc\n",
    "\n",
    "def run_model(datasets):\n",
    "    expression_dataset = datasets\n",
    "\n",
    "    saver = tf.train.Saver(write_version=tf.train.SaverDef.V1)\n",
    "\n",
    "    sess = tf.Session(config=\n",
    "        tf.ConfigProto(inter_op_parallelism_threads=parallelization_factor,\n",
    "                   intra_op_parallelism_threads=parallelization_factor))\n",
    "    \n",
    "    i_iter = 0\n",
    "\n",
    "    \n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    acc_count = 0\n",
    "    \n",
    "    train_acc = sess.run(accuracy, feed_dict={inputs: expression_dataset.train.unlabeled_ds.dataset, outputs: expression_dataset.train.unlabeled_ds.labels, training: False})\n",
    "    validation_acc = sess.run(accuracy, feed_dict={inputs: expression_dataset.validation.dataset, outputs: expression_dataset.validation.labels, training: False})\n",
    "    test_acc = sess.run(accuracy, feed_dict={inputs: expression_dataset.test.dataset, outputs: expression_dataset.test.labels, training: False})\n",
    "    \n",
    "    \n",
    "    print(\"INITIAL VALUES\")\n",
    "    _, pre_acc, _ = get_accuracies(\"Initial\", sess, expression_dataset)\n",
    "    \n",
    "    \n",
    "#     print('i_iter: ', i_iter, ' num_iter:', num_iter)\n",
    "#     print('num_examples', num_examples)\n",
    "\n",
    "#     for i in tqdm(range(i_iter, num_iter)):\n",
    "    for i in (range(i_iter, num_iter)):\n",
    "\n",
    "        dataset, labels = expression_dataset.train.next_batch(batch_size)\n",
    "        print(dataset.shape)\n",
    "        print(labels.shape)\n",
    "\n",
    "        sess.run(train_step, feed_dict={inputs: dataset, outputs: labels, training: True})\n",
    "\n",
    "        if (i > 1) and ((i+1) % (num_iter//num_epochs) == 0):\n",
    "            epoch_n = i//(num_examples//batch_size)\n",
    "            \n",
    "            _, curr_acc, _ = get_accuracies(\"Epoch(\" + str(epoch_n) + \")\", sess, expression_dataset)\n",
    "            \n",
    "            if curr_acc == pre_acc:\n",
    "                acc_count += 1\n",
    "            else :\n",
    "                acc_count = 0\n",
    "                \n",
    "            # TODO EARLY STOPPING\n",
    "            if acc_count > 3 and epoch_n > 30:\n",
    "                print(\"Early stop!!!!!\", acc_count, epoch_n)\n",
    "                break\n",
    "\n",
    "    get_accuracies(\"FINAL\", sess, expression_dataset)\n",
    "\n",
    "    y_p = tf.argmax(y, 1)\n",
    "    test_accuracy, y_pred = sess.run([accuracy,y_p], feed_dict={inputs: expression_dataset.test.dataset, outputs: expression_dataset.test.labels, training: False})\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    print (\"TEST accuracy:\", test_accuracy)\n",
    "    y_true = np.argmax(expression_dataset.test.labels,1)\n",
    "    print (\"Precision\", sk.metrics.precision_score(y_true, y_pred))\n",
    "    print (\"Recall\", sk.metrics.recall_score(y_true, y_pred))\n",
    "    print (\"f1_score\", sk.metrics.f1_score(y_true, y_pred))\n",
    "    print (\"confusion_matrix\")\n",
    "    print (sk.metrics.confusion_matrix(y_true, y_pred))\n",
    "    with open(file, \"a\") as text_file:\n",
    "        text_file.write(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % ('', \n",
    "                                                test_accuracy, \n",
    "                                                sk.metrics.f1_score(y_true, y_pred), \n",
    "                                                sk.metrics.precision_score(y_true, y_pred), \n",
    "                                                sk.metrics.recall_score(y_true, y_pred),\n",
    "                                                sk.metrics.confusion_matrix(y_true, y_pred).tolist()))\n",
    "\n",
    "\n",
    "    sess.close()\n",
    "#     raise ValueError('A very specific bad thing happened.')\n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     2,
     13,
     50
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1218, 20252)\n",
      "(730, 20252)\n",
      "n_classes 2\n",
      "INITIAL VALUES\n",
      "Initial  =>  train:  9.315068  validation:  9.4262295  test:  9.4262295  f1(test): 0.17228464419475656\n",
      "(120, 20252)\n",
      "(60, 2)\n",
      "(120, 20252)\n",
      "(60, 2)\n",
      "(120, 20252)\n",
      "(60, 2)\n",
      "(120, 20252)\n",
      "(60, 2)\n",
      "(120, 20252)\n",
      "(60, 2)\n",
      "(120, 20252)\n",
      "(60, 2)\n",
      "(120, 20252)\n",
      "(60, 2)\n",
      "(120, 20252)\n",
      "(60, 2)\n",
      "(120, 20252)\n",
      "(60, 2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-49de761754fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mdata_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-ccf81fa5f527>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "with open(file, \"w\") as text_file:\n",
    "    text_file.write(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % ('fold','accuracy', 'f1-score', 'precision', 'recall', 'conf_m'))\n",
    "    \n",
    "all_y_true = np.array([]).astype(int)\n",
    "all_y_pred = np.array([]).astype(int)\n",
    "\n",
    "print(Xnew.shape)\n",
    "\n",
    "\n",
    "# print('XXXXX',    len(Xnew), len(ynew))\n",
    "\n",
    "for train_valid_index, test_index in skf.split(Xnew, ynew[:,0]):\n",
    "    \n",
    "#     print('XXXXX', len(train_valid_index), len(test_index))\n",
    "\n",
    "\n",
    "    X_train_valid,  X_test = Xnew[train_valid_index], Xnew[test_index]\n",
    "    y_train_valid,  y_test = ynew[train_valid_index], ynew[test_index]\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid= train_test_split(X_train_valid, y_train_valid, test_size=0.25, stratify=y_train_valid[:,0])\n",
    "    \n",
    "#     from imblearn.over_sampling import RandomOverSampler \n",
    "#     ros = RandomOverSampler(random_state=0)\n",
    "#     X_train, y_train = ros.fit_sample(X_train, y_train[:,0])\n",
    "#     y_train = np.array(list(zip(y_train, 1- y_train)))\n",
    "    \n",
    "                         \n",
    "#     print('YYYYY', len(X_train), len(X_valid), len(X_test))\n",
    "#     print('ZZZZZ', len(y_train), len(y_valid), len(y_test))\n",
    "#     print('QQQQQ', sum(y_train[:,0]) / len(y_train), sum(y_valid[:,0]) / len(y_valid), sum(y_test[:,0]) / len(y_test))\n",
    "\n",
    "\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    data_sets.train = SemiDataSet(X_train,y_train , 60)\n",
    "    data_sets.validation = DataSet(X_valid,y_valid)\n",
    "    data_sets.test = DataSet(X_test,y_test)\n",
    "    \n",
    "    y_true, y_pred = run_model(data_sets)\n",
    "    \n",
    "\n",
    "    all_y_true = np.append(all_y_true,y_true)\n",
    "    all_y_pred = np.append(all_y_pred,y_pred)\n",
    "    \n",
    "\n",
    "\n",
    "#     break\n",
    "print (sk.metrics.confusion_matrix(all_y_true, all_y_pred))\n",
    "with open(file, \"a\") as text_file:\n",
    "    text_file.write(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % ('ALL', \n",
    "                        sk.metrics.accuracy_score(all_y_true, all_y_pred), \n",
    "                        sk.metrics.f1_score(all_y_true, all_y_pred), \n",
    "                        sk.metrics.precision_score(all_y_true, all_y_pred), \n",
    "                        sk.metrics.recall_score(all_y_true, all_y_pred),\n",
    "                        sk.metrics.confusion_matrix(all_y_true, all_y_pred).tolist())\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk.metrics.confusion_matrix(all_y_true, all_y_pred).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxxx = X.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxxx = maxxx[maxxx!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxxx.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tcga_path = \"/home/nanni/Data/TCGA/Xena/tcga.tsv\"\n",
    "tcga = pd.read_csv(tcga_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = tcga[tcga.columns[7:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.mean(axis=1).shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
